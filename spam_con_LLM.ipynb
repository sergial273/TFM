{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detección de Spam con BERT (LLM)\n",
    "Este cuaderno demuestra el proceso de construcción y evaluación de un modelo basado en BERT (Bidirectional Encoder Representations from Transformers) para la detección de spam.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introducción a los Modelos de Lenguaje (LLM)\n",
    "Los Modelos de Lenguaje (LLM, por sus siglas en inglés) son redes neuronales profundas entrenadas en grandes cantidades de datos de texto para comprender y generar lenguaje humano. BERT es un tipo de LLM desarrollado por Google que ha revolucionado el procesamiento del lenguaje natural (NLP).\n",
    "\n",
    "**Características clave de BERT:**\n",
    "- **Bidireccional:** Analiza el contexto de una palabra considerando tanto lo que viene antes como después.\n",
    "\n",
    "- **Basado en Transformers:** Utiliza la arquitectura Transformer con mecanismos de atención.\n",
    "\n",
    "- **Pre-entrenamiento:** Se entrena primero en tareas generales de lenguaje antes de ser ajustado para tareas específicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Importación de librerías\n",
    "Importamos las librerías necesarias para trabajar con BERT y evaluar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tratarDatos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Carga y preprocesamiento de datos\n",
    "Cargamos los datos y los preparamos para el modelo BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos (usando el mismo formato que en los cuadernos anteriores)\n",
    "xtrain, xtest, ytrain, ytest = tratarDatos.combinar_y_dividir_datos()\n",
    "\n",
    "# Convertir a DataFrame para facilitar el manejo\n",
    "train_df = pd.DataFrame({'text': xtrain, 'label': ytrain})\n",
    "test_df = pd.DataFrame({'text': xtest, 'label': ytest})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Tokenización con BERT\n",
    "BERT requiere una tokenización especial que incluye tokens especiales como [CLS] y [SEP]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el tokenizador de BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Función para tokenizar los textos\n",
    "def tokenize_texts(texts, max_length=128):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Tokenizar los conjuntos de entrenamiento y prueba\n",
    "train_encodings = tokenize_texts(train_df['text'])\n",
    "test_encodings = tokenize_texts(test_df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Creación del Dataset para PyTorch\n",
    "Creamos una clase personalizada para manejar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Crear los datasets\n",
    "train_dataset = SpamDataset(train_encodings, train_df['label'].values)\n",
    "test_dataset = SpamDataset(test_encodings, test_df['label'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Configuración del modelo BERT\n",
    "Cargamos el modelo pre-entrenado y configuramos los parámetros de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Cargar el modelo BERT para clasificación de secuencias\n",
    "modelo = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2  # Spam o Ham\n",
    ")\n",
    "\n",
    "# Configurar los argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Directorio de salida\n",
    "    num_train_epochs=3,              # Número de épocas\n",
    "    per_device_train_batch_size=8,   # Tamaño del lote por dispositivo durante el entrenamiento\n",
    "    per_device_eval_batch_size=16,   # Tamaño del lote por dispositivo durante la evaluación\n",
    "    warmup_steps=500,                # Número de pasos de calentamiento\n",
    "    weight_decay=0.01,               # Penalización de peso\n",
    "    logging_dir='./logs',            # Directorio para logs\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\"      # Evaluar al final de cada época\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Entrenamiento del modelo\n",
    "Entrenamos el modelo BERT utilizando el Trainer de Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3001' max='13575' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3001/13575 3:17:12 < 11:35:20, 0.25 it/s, Epoch 0.66/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Crear el Trainer\n",
    "trainer = Trainer(\n",
    "    model=modelo,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Comenzar el entrenamiento\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Evaluación del modelo\n",
    "Evaluamos el modelo utilizando métricas estándar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"resultados/LLM\", exist_ok=True)\n",
    "\n",
    "# Función para evaluar el modelo\n",
    "def evaluate_model(model, dataset):\n",
    "    predicciones = trainer.predict(dataset)\n",
    "    preds = np.argmax(predicciones.predictions, axis=1)\n",
    "    \n",
    "    # Métricas\n",
    "    precision = metrics.accuracy_score(dataset.labels, preds)\n",
    "    print(f\"Puntuación de precisión: {precision:.4f}\")\n",
    "    \n",
    "    # Matriz de confusión\n",
    "    cm = metrics.confusion_matrix(dataset.labels, preds)\n",
    "    plt.figure()\n",
    "    metrics.ConfusionMatrixDisplay(cm, display_labels=['Ham', 'Spam']).plot()\n",
    "    plt.title('Matriz de Confusión - BERT')\n",
    "    plt.savefig('resultados/LLM/BERT_confusion_matrix.png')\n",
    "    \n",
    "    # Reporte de clasificación\n",
    "    print(metrics.classification_report(dataset.labels, preds, target_names=['Ham', 'Spam']))\n",
    "    \n",
    "    # Curva ROC (solo para modelos que devuelven probabilidades)\n",
    "    probs = torch.softmax(torch.tensor(predicciones.predictions), dim=1)[:, 1].numpy()\n",
    "    fpr, tpr, _ = metrics.roc_curve(dataset.labels, probs)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'Curva ROC (área = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Tasa de Falsos Positivos')\n",
    "    plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "    plt.title('Curva ROC - BERT')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('resultados/LLM/BERT_roc_curve.png')\n",
    "\n",
    "# Evaluar el modelo\n",
    "evaluate_model(modelo, test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
